{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Class Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Criterion:\n",
    "    def __init__(self, feature, threshold, target):\n",
    "        self.feature = feature\n",
    "        self.threshold = threshold\n",
    "        self.target = target\n",
    "    \n",
    "    def get_feature(self):\n",
    "        return self.feature\n",
    "    \n",
    "    def get_threshold(self):\n",
    "        return self.threshold\n",
    "    \n",
    "    def get_target(self):\n",
    "        return self.target\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, criterion):\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "        self.criterion = criterion\n",
    "        \n",
    "    def set_left(self, node):\n",
    "        self.left = node\n",
    "        \n",
    "    def set_right(self, node):\n",
    "        self.right = node\n",
    "        \n",
    "    def set_criterion(self, criterion):\n",
    "        self.criterion = criterion\n",
    "\n",
    "    def get_left(self):\n",
    "        return self.left\n",
    "\n",
    "    def get_right(self):\n",
    "        return self.right\n",
    "\n",
    "    def get_criterion(self):\n",
    "        return self.criterion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Function Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the best splitting asset\n",
    "Iterate through all available features.  \n",
    "Take median value as the threshold for each feature.  \n",
    "Find the smallest impurity value and return it's feature and threshold value.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_split_class(x, y, features, targets):\n",
    "    imp_best = 1\n",
    "    feature_best = features[0]\n",
    "    threshold_best = 0\n",
    "    # iterate through features\n",
    "    for feature in features:\n",
    "        # sort possible values of the selected feature\n",
    "        vals = x[feature]\n",
    "        vals = sorted(set(vals))\n",
    "        # for i in range(1, len(vals)):\n",
    "        # threshold = (vals[i - 1] + vals[i]) / 2.0\n",
    "        # select median as the threshold\n",
    "        threshold = pd.Series(vals).median()\n",
    "        # iterate through targets\n",
    "        for target in range(targets):\n",
    "            d1pos = 0\n",
    "            d1neg = 0\n",
    "            d2pos = 0\n",
    "            d2neg = 0\n",
    "            # count positive and negative instances in 2 subsets\n",
    "            for index, row in x.iterrows():\n",
    "                #print([index, row[feature], threshold, y[index]])\n",
    "                if row[feature] < threshold:\n",
    "                    if y[index] == target:\n",
    "                        d1pos = d1pos + 1\n",
    "                    else:\n",
    "                        d1neg = d1neg + 1\n",
    "                else:\n",
    "                    if y[index] == target:\n",
    "                        d2pos = d2pos + 1\n",
    "                    else:\n",
    "                        d2neg = d2neg + 1\n",
    "            # if target count is 0, skip\n",
    "            if d1pos + d2pos == 0 or d1neg + d2neg == 0:\n",
    "                continue\n",
    "            # calculate entropy for 2 subsets\n",
    "            d1 = d1pos + d1neg\n",
    "            d2 = d2pos + d2neg\n",
    "            if d1 == 0 or d2 == 0:\n",
    "                continue\n",
    "            if d1pos == 0 or d1neg == 0:\n",
    "                imp1 = 0\n",
    "            else:\n",
    "                imp1 = -(d1pos / d1) * math.log(d1pos / d1, 2) - (d1neg / d1) * math.log(d1neg / d1, 2)\n",
    "            if d2pos == 0 or d2neg == 0:\n",
    "                imp2 = 0\n",
    "            else:\n",
    "                imp2 = -(d2pos / d2) * math.log(d2pos / d2, 2) - (d2neg / d2) * math.log(d2neg / d2, 2)\n",
    "            # calculate new entopy for that entire set\n",
    "            imp = (d1 / (d1 + d2)) * imp1 + (d2 / (d1 + d2)) * imp2\n",
    "            # check for minimum entropy\n",
    "            if imp < imp_best:\n",
    "                # update criterion\n",
    "                imp_best = imp\n",
    "                feature_best = feature\n",
    "                threshold_best = threshold\n",
    "    return [feature_best, threshold_best]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grow tree recursively\n",
    "If x is homogeneous, add a new leaf node to the decision tree.  \n",
    "If maximum tree depth is reached, add a new leaf node with the most frequent value in y.  \n",
    "Otherwise, find the best splitting feature, add it to the decision tree and return the tree.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grow_tree(x, y, features, targets, height):\n",
    "    # check homogeneity\n",
    "    if y.nunique() == 1:\n",
    "        return Node(Criterion(None, None, y[0]))\n",
    "    \n",
    "    # check height\n",
    "    if height == 0:\n",
    "        return Node(Criterion(None, None, y.value_counts().idxmax()))\n",
    "    \n",
    "    # get best split criterion\n",
    "    [feature, threshold] = best_split_class(x, y, features, targets)\n",
    "    node = Node(Criterion(feature, threshold, -1))\n",
    "    \n",
    "    # split according to the criterion\n",
    "    x1 = pd.DataFrame(columns = list(x.columns))\n",
    "    x2 = pd.DataFrame(columns = list(x.columns))\n",
    "    y1 = pd.Series([], dtype=np.float64)\n",
    "    y2 = pd.Series([], dtype=np.float64)\n",
    "    for index, row in x.iterrows():\n",
    "        if row[feature] < threshold:\n",
    "            x1 = x1.append(row, ignore_index=True)\n",
    "            y1 = y1.append(pd.Series([y[index]], dtype=np.float64), ignore_index=True)\n",
    "        else:\n",
    "            x2 = x2.append(row, ignore_index=True)\n",
    "            y2 = y2.append(pd.Series([y[index]], dtype=np.float64), ignore_index=True)\n",
    "            \n",
    "    # if left set is not empty\n",
    "    if not x1.empty:\n",
    "        node.set_left(grow_tree(x1, y1, features, targets, height - 1))\n",
    "    if not x2.empty:\n",
    "        node.set_right(grow_tree(x2, y2, features, targets, height - 1))\n",
    "    return node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train from the given data and target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(x, y, height):\n",
    "    return grow_tree(x, y, list(x.columns), y.nunique(), height)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict from the given data and decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(node, x):\n",
    "    # check if it is a leaf node\n",
    "    target = node.get_criterion().get_target()\n",
    "    if target != -1:\n",
    "        x['result'] = [target] * len(x.index)\n",
    "        return x\n",
    "    \n",
    "    # split set according to the criterion\n",
    "    x1 = pd.DataFrame(columns = list(x.columns))\n",
    "    x2 = pd.DataFrame(columns = list(x.columns))\n",
    "    feature = node.get_criterion().get_feature()\n",
    "    threshold = node.get_criterion().get_threshold()\n",
    "    for index, row in x.iterrows():\n",
    "        if (row[feature] < threshold):\n",
    "            x1 = x1.append(row, ignore_index=True)\n",
    "        else:\n",
    "            x2 = x2.append(row, ignore_index=True)\n",
    "            \n",
    "    # predict left subset\n",
    "    if node.get_left() is not None and not x1.empty:\n",
    "        x1 = predict(node.get_left(), x1)\n",
    "    # predict right subset\n",
    "    if node.get_right() is not None and not x2.empty:\n",
    "        x2 = predict(node.get_right(), x2)\n",
    "        \n",
    "    # return 2 predicted subsets combined\n",
    "    return x1.append(x2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Predict Titanic Data Set\n",
    "'Name' field is removed from the data set in preprocessing.  \n",
    "'Sex' field is categorical in the data set, which is converted to integers (0/1) in preprocessing.  \n",
    "The data set does not contain any N/A values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 62.90550070521862, 55.61797752808989]\n",
      "[5, 81.80535966149506, 76.40449438202246]\n",
      "[10, 91.3963328631876, 77.52808988764045]\n",
      "[15, 97.03808180535967, 75.84269662921348]\n",
      "[20, 97.32016925246828, 75.28089887640449]\n"
     ]
    }
   ],
   "source": [
    "# get data and preprocess it\n",
    "df = pd.read_csv('titanic.csv')\n",
    "df = df.loc[:, ~df.columns.str.contains('^Unnamed')]\n",
    "df = df.drop(['Name'],axis=1)\n",
    "mapping = {'male': 0, 'female': 1}\n",
    "df['Sex'] = df['Sex'].map(mapping)\n",
    "df.applymap(lambda s: mapping.get(s) if s in mapping else s)\n",
    "\n",
    "# format training data for decision-tree\n",
    "x = df.drop(['Survived'],axis=1)\n",
    "y = df['Survived']\n",
    "\n",
    "# split train and test data (80/20)\n",
    "mask = np.random.rand(len(x)) < 0.8\n",
    "train_x = x[mask]\n",
    "train_y = y[mask]\n",
    "test_x = x[~mask]\n",
    "test_y = y[~mask]\n",
    "\n",
    "### train & predict at various tree depth\n",
    "for depth in range(0, 21, 5):\n",
    "    ### train\n",
    "    columns = list(x.columns)\n",
    "    tree = train(train_x, train_y, depth)\n",
    "    \n",
    "    ### predict using train data\n",
    "    data1 = train_x.copy()\n",
    "    data1['target'] = train_y.tolist()\n",
    "    data2 = predict(tree, train_x.copy())\n",
    "    # sort for comparison\n",
    "    data1 = data1.sort_values(by=columns)\n",
    "    data2 = data2.sort_values(by=columns)\n",
    "    # get target and prediction\n",
    "    target = data1['target'].tolist()\n",
    "    result = data2['result'].tolist()\n",
    "    # compare and get the accuracy\n",
    "    count_pos = 0\n",
    "    count_neg = 0\n",
    "    for i in range(len(target)): \n",
    "        if (target[i] == result[i]):\n",
    "            count_pos = count_pos + 1\n",
    "        else:\n",
    "            count_neg = count_neg + 1\n",
    "    accuracy_train = count_pos / (count_pos + count_neg) * 100\n",
    "\n",
    "    ### predict using test data\n",
    "    data1 = test_x.copy()\n",
    "    data1['target'] = test_y.tolist()\n",
    "    data2 = predict(tree, test_x.copy())\n",
    "    # sort for comparison\n",
    "    data1 = data1.sort_values(by=columns)\n",
    "    data2 = data2.sort_values(by=columns)\n",
    "    # get target and prediction\n",
    "    target = data1['target'].tolist()\n",
    "    result = data2['result'].tolist()\n",
    "    # compare and get the accuracy\n",
    "    count_pos = 0\n",
    "    count_neg = 0\n",
    "    for i in range(len(target)): \n",
    "        if (target[i] == result[i]):\n",
    "            count_pos = count_pos + 1\n",
    "        else:\n",
    "            count_neg = count_neg + 1\n",
    "    accuracy_test = count_pos / (count_pos + count_neg) * 100\n",
    "\n",
    "    # print accuracy\n",
    "    print([depth, accuracy_train, accuracy_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy Report\n",
    "When the tree depth is increased from 0 to 20, prediction accuracy is increased for training data, whereas it is decreased for test data at 15 and onwards.  \n",
    "This indicates that overfitting becomes apparent at tree depth ~10.  \n",
    "In grow_tree() function, the median is always selected for the threshold.  \n",
    "Instead of using the median value, a more sophisticated approach may be implemented to get better results at smaller tree depths."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Predict Car Evaluation Data Set\n",
    "Car evaluation data set contains categorical fields only, which should all be converted to integers in preprocessing.  \n",
    "No N/A values are found in the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 69.67509025270758, 71.42857142857143]\n",
      "[5, 84.04332129963899, 82.79883381924198]\n",
      "[10, 99.49458483754513, 97.667638483965]\n",
      "[15, 100.0, 97.95918367346938]\n",
      "[20, 100.0, 97.95918367346938]\n"
     ]
    }
   ],
   "source": [
    "# get data and preprocess\n",
    "df = pd.read_csv('car.csv')\n",
    "df = df.loc[:, ~df.columns.str.contains('^Unnamed')]\n",
    "\n",
    "# convert categorical 'buying', 'maint' fields to integers\n",
    "mapping = {'vhigh': 0, 'high': 1, 'med': 2, 'low': 3}\n",
    "df['buying'] = df['buying'].map(mapping)\n",
    "df['maint'] = df['maint'].map(mapping)\n",
    "df.applymap(lambda s: mapping.get(s) if s in mapping else s)\n",
    "\n",
    "# convert categorical 'doors' fields to integers\n",
    "mapping = {'2': 0, '3': 1, '4': 2, '5more': 3}\n",
    "df['doors'] = df['doors'].map(mapping)\n",
    "df.applymap(lambda s: mapping.get(s) if s in mapping else s)\n",
    "\n",
    "# convert categorical 'persons' fields to integers\n",
    "mapping = {'2': 0, '4': 1, 'more': 2}\n",
    "df['persons'] = df['persons'].map(mapping)\n",
    "df.applymap(lambda s: mapping.get(s) if s in mapping else s)\n",
    "\n",
    "# convert categorical 'lug_boot' fields to integers\n",
    "mapping = {'small': 0, 'med': 1, 'big': 2}\n",
    "df['lug_boot'] = df['lug_boot'].map(mapping)\n",
    "df.applymap(lambda s: mapping.get(s) if s in mapping else s)\n",
    "\n",
    "# convert categorical 'safety' fields to integers\n",
    "mapping = {'low': 0, 'med': 1, 'high': 2}\n",
    "df['safety'] = df['safety'].map(mapping)\n",
    "df.applymap(lambda s: mapping.get(s) if s in mapping else s)\n",
    "\n",
    "# convert categorical 'class' fields to integers\n",
    "mapping = {'unacc': 0, 'acc': 1, 'good': 2, 'vgood': 3}\n",
    "df['class'] = df['class'].map(mapping)\n",
    "df.applymap(lambda s: mapping.get(s) if s in mapping else s)\n",
    "\n",
    "# format training data for decision-tree\n",
    "x = df.drop(['class'],axis=1)\n",
    "y = df['class']\n",
    "\n",
    "# split train and test data (80/20)\n",
    "mask = np.random.rand(len(x)) < 0.8\n",
    "train_x = x[mask]\n",
    "train_y = y[mask]\n",
    "test_x = x[~mask]\n",
    "test_y = y[~mask]\n",
    "\n",
    "### train & predict at various tree depth\n",
    "for depth in range(0, 21, 5):\n",
    "    ### train\n",
    "    columns = list(x.columns)\n",
    "    tree = train(train_x, train_y, depth)\n",
    "    \n",
    "    ### predict using train data\n",
    "    data1 = train_x.copy()\n",
    "    data1['target'] = train_y.tolist()\n",
    "    data2 = predict(tree, train_x.copy())\n",
    "    # sort for comparison\n",
    "    data1 = data1.sort_values(by=columns)\n",
    "    data2 = data2.sort_values(by=columns)\n",
    "    # get target and prediction\n",
    "    target = data1['target'].tolist()\n",
    "    result = data2['result'].tolist()\n",
    "    # compare and get the accuracy\n",
    "    count_pos = 0\n",
    "    count_neg = 0\n",
    "    for i in range(len(target)): \n",
    "        if (target[i] == result[i]):\n",
    "            count_pos = count_pos + 1\n",
    "        else:\n",
    "            count_neg = count_neg + 1\n",
    "    accuracy_train = count_pos / (count_pos + count_neg) * 100\n",
    "\n",
    "    ### predict using test data\n",
    "    data1 = test_x.copy()\n",
    "    data1['target'] = test_y.tolist()\n",
    "    data2 = predict(tree, test_x.copy())\n",
    "    # sort for comparison\n",
    "    data1 = data1.sort_values(by=columns)\n",
    "    data2 = data2.sort_values(by=columns)\n",
    "    # get target and prediction\n",
    "    target = data1['target'].tolist()\n",
    "    result = data2['result'].tolist()\n",
    "    # compare and get the accuracy\n",
    "    count_pos = 0\n",
    "    count_neg = 0\n",
    "    for i in range(len(target)): \n",
    "        if (target[i] == result[i]):\n",
    "            count_pos = count_pos + 1\n",
    "        else:\n",
    "            count_neg = count_neg + 1\n",
    "    accuracy_test = count_pos / (count_pos + count_neg) * 100\n",
    "\n",
    "    # print accuracy\n",
    "    print([depth, accuracy_train, accuracy_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy Report\n",
    "When the tree depth is increased from 0 to 20, prediction accuracy is increased for both the train and test data, and it saturated at 15 and onwards, which may indicate that the overfitting is not so apparent compared to the Titanic data set.  \n",
    "Compared to Titanic data set which contains discrete values, Car data set contains categorical values only, which may have contributed to higher accuracies at similar tree depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
